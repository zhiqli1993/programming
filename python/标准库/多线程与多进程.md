# Python多线程与多进程编程

Python提供了多种方式来实现并发编程，主要包括多线程（threading）和多进程（multiprocessing）。本文详细介绍这两种并发编程方法的基本概念、使用方法和最佳实践。

## 线程与进程的基本概念

### 什么是线程和进程

- **进程（Process）**：是操作系统分配资源的基本单位，每个进程都有自己独立的内存空间和系统资源。
- **线程（Thread）**：是进程内的执行单元，同一进程中的多个线程共享该进程的内存空间和资源。

### Python中的GIL（全局解释器锁）

Python的标准实现CPython有一个名为GIL（Global Interpreter Lock，全局解释器锁）的机制，它确保同一时刻只有一个线程可以执行Python字节码。这意味着：

- 在CPU密集型任务中，多线程不会带来性能提升，有时甚至会因为线程切换开销而降低性能
- 在I/O密集型任务中，由于线程在等待I/O时会释放GIL，多线程仍然可以提高程序效率
- 如果要充分利用多核CPU的计算能力，应该使用多进程而非多线程

## Python的threading模块

### 创建和启动线程

最基本的线程创建方式是使用`threading.Thread`类：

```python
import threading
import time

def worker(name):
    """线程工作函数"""
    print(f"线程 {name} 开始工作")
    time.sleep(2)  # 模拟工作耗时
    print(f"线程 {name} 工作完成")

# 创建线程
threads = []
for i in range(5):
    t = threading.Thread(target=worker, args=(f"Thread-{i}",))
    threads.append(t)
    t.start()  # 启动线程

# 等待所有线程完成
for t in threads:
    t.join()

print("所有线程工作完成")
```

### 使用Thread子类创建线程

可以通过继承`threading.Thread`类来创建自定义线程类：

```python
import threading
import time

class WorkerThread(threading.Thread):
    def __init__(self, name):
        super().__init__()
        self.name = name
    
    def run(self):
        """线程执行的方法"""
        print(f"线程 {self.name} 开始工作")
        time.sleep(2)  # 模拟工作耗时
        print(f"线程 {self.name} 工作完成")

# 创建线程
threads = []
for i in range(5):
    t = WorkerThread(f"Thread-{i}")
    threads.append(t)
    t.start()  # 启动线程

# 等待所有线程完成
for t in threads:
    t.join()

print("所有线程工作完成")
```

### 线程同步机制

当多个线程访问共享资源时，需要使用同步机制来避免数据竞争和不一致性。

#### 使用Lock（锁）

`threading.Lock`提供了最基本的线程同步机制：

```python
import threading
import time

# 共享资源
counter = 0
counter_lock = threading.Lock()

def increment_counter(count, thread_name):
    global counter
    
    for _ in range(count):
        # 获取锁
        counter_lock.acquire()
        try:
            # 修改共享资源
            current = counter
            time.sleep(0.001)  # 模拟一些处理时间
            counter = current + 1
            print(f"{thread_name}: 计数器 = {counter}")
        finally:
            # 释放锁
            counter_lock.release()

# 创建线程
threads = []
for i in range(5):
    t = threading.Thread(target=increment_counter, args=(10, f"Thread-{i}"))
    threads.append(t)
    t.start()

# 等待所有线程完成
for t in threads:
    t.join()

print(f"最终计数器值: {counter}")
```

也可以使用上下文管理器（with语句）来自动获取和释放锁：

```python
def increment_counter(count, thread_name):
    global counter
    
    for _ in range(count):
        with counter_lock:  # 自动获取和释放锁
            current = counter
            time.sleep(0.001)
            counter = current + 1
            print(f"{thread_name}: 计数器 = {counter}")
```

#### 使用RLock（可重入锁）

当一个函数获取了锁，然后调用另一个也需要获取相同锁的函数时，普通的`Lock`会导致死锁。这时候可以使用`RLock`（可重入锁）：

```python
import threading

class SharedResource:
    def __init__(self):
        self.value = 0
        self.lock = threading.RLock()  # 使用可重入锁
    
    def update(self, new_value):
        with self.lock:
            self.value = new_value
            self.display()  # 调用另一个需要获取锁的方法
    
    def display(self):
        with self.lock:  # 同一线程可以再次获取锁
            print(f"当前值: {self.value}")

# 使用共享资源
resource = SharedResource()

def worker(name, value):
    print(f"线程 {name} 开始更新资源")
    resource.update(value)
    print(f"线程 {name} 更新完成")

# 创建线程
threads = []
for i in range(5):
    t = threading.Thread(target=worker, args=(f"Thread-{i}", i * 10))
    threads.append(t)
    t.start()

# 等待所有线程完成
for t in threads:
    t.join()
```

#### 使用Condition（条件变量）

`Condition`对象用于线程间的通知，当某些状态变化时通知等待的线程：

```python
import threading
import time
import random

# 模拟生产者-消费者问题
class Buffer:
    def __init__(self, size):
        self.size = size
        self.buffer = []
        self.condition = threading.Condition()
    
    def put(self, item):
        with self.condition:
            # 当缓冲区满时等待
            while len(self.buffer) >= self.size:
                print(f"缓冲区已满，生产者等待...")
                self.condition.wait()
            
            # 添加项目到缓冲区
            self.buffer.append(item)
            print(f"生产者添加: {item}, 缓冲区: {self.buffer}")
            
            # 通知消费者
            self.condition.notify()
    
    def get(self):
        with self.condition:
            # 当缓冲区空时等待
            while not self.buffer:
                print("缓冲区为空，消费者等待...")
                self.condition.wait()
            
            # 从缓冲区取出项目
            item = self.buffer.pop(0)
            print(f"消费者取出: {item}, 缓冲区: {self.buffer}")
            
            # 通知生产者
            self.condition.notify()
            return item

# 创建共享缓冲区
buffer = Buffer(5)

# 生产者函数
def producer():
    for i in range(10):
        item = f"项目-{i}"
        buffer.put(item)
        time.sleep(random.uniform(0.1, 0.5))  # 随机延时

# 消费者函数
def consumer():
    for i in range(10):
        item = buffer.get()
        time.sleep(random.uniform(0.2, 0.6))  # 随机延时

# 创建线程
producer_thread = threading.Thread(target=producer)
consumer_thread = threading.Thread(target=consumer)

# 启动线程
producer_thread.start()
consumer_thread.start()

# 等待完成
producer_thread.join()
consumer_thread.join()
```

#### 使用Semaphore（信号量）

`Semaphore`用于限制同时访问资源的线程数量：

```python
import threading
import time
import random

# 创建信号量，限制最多3个线程同时访问资源
semaphore = threading.Semaphore(3)

def worker(name):
    print(f"线程 {name} 正在尝试访问资源")
    
    with semaphore:
        print(f"线程 {name} 获得了资源访问权")
        time.sleep(random.uniform(1, 3))  # 模拟使用资源
        print(f"线程 {name} 释放了资源")

# 创建线程
threads = []
for i in range(10):
    t = threading.Thread(target=worker, args=(f"Thread-{i}",))
    threads.append(t)
    t.start()

# 等待所有线程完成
for t in threads:
    t.join()
```

#### 使用Event（事件）

`Event`对象用于线程间的简单通信，一个线程等待事件发生，另一个线程触发事件：

```python
import threading
import time

# 创建事件对象
event = threading.Event()

def waiter(name):
    print(f"{name} 等待事件...")
    event.wait()  # 等待事件被设置
    print(f"{name} 收到事件通知！")

def setter():
    print("准备设置事件...")
    time.sleep(3)  # 等待一段时间
    print("设置事件，通知所有等待的线程")
    event.set()  # 设置事件，通知所有等待的线程

# 创建线程
threads = []
for i in range(5):
    t = threading.Thread(target=waiter, args=(f"Waiter-{i}",))
    threads.append(t)
    t.start()

setter_thread = threading.Thread(target=setter)
setter_thread.start()

# 等待所有线程完成
for t in threads:
    t.join()
setter_thread.join()
```

#### 使用Barrier（栅栏）

`Barrier`用于同步一组线程，使它们在某个点上等待，直到所有线程都到达该点：

```python
import threading
import time
import random

# 创建栅栏，等待5个线程
barrier = threading.Barrier(5)

def worker(name):
    print(f"线程 {name} 开始工作")
    time.sleep(random.uniform(0.5, 2.5))  # 随机工作时间
    
    print(f"线程 {name} 到达第一阶段检查点，等待其他线程...")
    barrier.wait()  # 等待所有线程到达
    
    print(f"线程 {name} 继续执行第二阶段")
    time.sleep(random.uniform(0.5, 2.5))  # 随机工作时间
    
    print(f"线程 {name} 到达第二阶段检查点，等待其他线程...")
    barrier.wait()  # 等待所有线程到达
    
    print(f"线程 {name} 完成所有工作")

# 创建线程
threads = []
for i in range(5):
    t = threading.Thread(target=worker, args=(f"Thread-{i}",))
    threads.append(t)
    t.start()

# 等待所有线程完成
for t in threads:
    t.join()

print("所有线程都完成了工作")
```

### 线程池

虽然Python标准库中没有专门的线程池实现，但可以使用`concurrent.futures`模块的`ThreadPoolExecutor`：

```python
import concurrent.futures
import time
import random

def task(name):
    print(f"任务 {name} 开始执行")
    time.sleep(random.uniform(0.5, 2.5))  # 随机工作时间
    result = f"任务 {name} 的结果"
    print(f"任务 {name} 执行完成")
    return result

# 创建线程池，最多使用5个线程
with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
    # 提交10个任务
    futures = {executor.submit(task, f"Task-{i}"): i for i in range(10)}
    
    # 获取结果，按完成顺序
    for future in concurrent.futures.as_completed(futures):
        task_id = futures[future]
        try:
            result = future.result()
            print(f"任务 {task_id} 返回结果: {result}")
        except Exception as e:
            print(f"任务 {task_id} 生成异常: {e}")

# 使用map方法
with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
    tasks = [f"Task-{i}" for i in range(10)]
    results = executor.map(task, tasks)
    
    # 结果按提交顺序返回
    for task_id, result in enumerate(results):
        print(f"任务 {task_id} 的map结果: {result}")
```

### 线程局部数据

`threading.local()`提供了线程局部存储，每个线程都有独立的数据副本：

```python
import threading
import random

# 创建线程局部存储
local_data = threading.local()

def worker(name):
    # 设置线程局部数据
    local_data.value = random.randint(1, 100)
    local_data.name = name
    
    print(f"线程 {name} 的局部数据: value={local_data.value}, name={local_data.name}")
    
    process_data()

def process_data():
    # 在另一个函数中访问线程局部数据
    # 每个线程看到的都是自己设置的值
    print(f"处理数据: value={local_data.value}, name={local_data.name}")

# 创建线程
threads = []
for i in range(5):
    t = threading.Thread(target=worker, args=(f"Thread-{i}",))
    threads.append(t)
    t.start()

# 等待所有线程完成
for t in threads:
    t.join()
```

## Python的multiprocessing模块

### 创建和启动进程

`multiprocessing`模块的API设计与`threading`模块类似：

```python
import multiprocessing
import time
import os

def worker(name):
    """进程工作函数"""
    print(f"进程 {name} (PID: {os.getpid()}) 开始工作")
    time.sleep(2)  # 模拟工作耗时
    print(f"进程 {name} (PID: {os.getpid()}) 工作完成")

if __name__ == "__main__":
    # 显示主进程ID
    print(f"主进程 PID: {os.getpid()}")
    
    # 创建进程
    processes = []
    for i in range(5):
        p = multiprocessing.Process(target=worker, args=(f"Process-{i}",))
        processes.append(p)
        p.start()  # 启动进程
    
    # 等待所有进程完成
    for p in processes:
        p.join()
    
    print("所有进程工作完成")
```

### 使用Process子类创建进程

可以通过继承`multiprocessing.Process`类来创建自定义进程类：

```python
import multiprocessing
import time
import os

class WorkerProcess(multiprocessing.Process):
    def __init__(self, name):
        super().__init__()
        self.name = name
    
    def run(self):
        """进程执行的方法"""
        print(f"进程 {self.name} (PID: {os.getpid()}) 开始工作")
        time.sleep(2)  # 模拟工作耗时
        print(f"进程 {self.name} (PID: {os.getpid()}) 工作完成")

if __name__ == "__main__":
    # 显示主进程ID
    print(f"主进程 PID: {os.getpid()}")
    
    # 创建进程
    processes = []
    for i in range(5):
        p = WorkerProcess(f"Process-{i}")
        processes.append(p)
        p.start()  # 启动进程
    
    # 等待所有进程完成
    for p in processes:
        p.join()
    
    print("所有进程工作完成")
```

### 进程间通信

由于每个进程有自己独立的内存空间，进程间不能像线程那样直接共享变量。`multiprocessing`模块提供了多种进程间通信机制。

#### 使用Queue进行通信

```python
import multiprocessing
import time
import random

def producer(queue):
    """生产者函数"""
    for i in range(10):
        item = f"项目-{i}"
        queue.put(item)  # 将项目放入队列
        print(f"生产者添加: {item}")
        time.sleep(random.uniform(0.1, 0.5))  # 随机延时

def consumer(queue):
    """消费者函数"""
    while True:
        try:
            item = queue.get(timeout=3)  # 从队列获取项目，设置超时
            print(f"消费者获取: {item}")
            time.sleep(random.uniform(0.2, 0.6))  # 随机延时
        except queue.Empty:
            print("队列为空，消费者退出")
            break

if __name__ == "__main__":
    # 创建进程间共享的队列
    queue = multiprocessing.Queue()
    
    # 创建生产者和消费者进程
    producer_proc = multiprocessing.Process(target=producer, args=(queue,))
    consumer_proc = multiprocessing.Process(target=consumer, args=(queue,))
    
    # 启动进程
    producer_proc.start()
    consumer_proc.start()
    
    # 等待进程完成
    producer_proc.join()
    consumer_proc.join()
    
    print("所有进程完成")
```

#### 使用Pipe进行通信

`Pipe`提供了两个进程之间的双向通信通道：

```python
import multiprocessing
import time

def sender(conn):
    """发送数据的进程"""
    print("发送进程开始")
    for i in range(10):
        msg = f"消息-{i}"
        conn.send(msg)  # 发送消息
        print(f"发送: {msg}")
        time.sleep(0.5)
    
    # 发送终止信号
    conn.send(None)
    conn.close()
    print("发送进程结束")

def receiver(conn):
    """接收数据的进程"""
    print("接收进程开始")
    while True:
        msg = conn.recv()  # 接收消息
        if msg is None:  # 检查终止信号
            break
        print(f"接收: {msg}")
    
    conn.close()
    print("接收进程结束")

if __name__ == "__main__":
    # 创建管道
    parent_conn, child_conn = multiprocessing.Pipe()
    
    # 创建进程
    sender_proc = multiprocessing.Process(target=sender, args=(parent_conn,))
    receiver_proc = multiprocessing.Process(target=receiver, args=(child_conn,))
    
    # 启动进程
    sender_proc.start()
    receiver_proc.start()
    
    # 等待进程完成
    sender_proc.join()
    receiver_proc.join()
    
    print("所有进程完成")
```

#### 使用Manager共享数据

`Manager`对象提供了一种在进程间共享Python对象（如列表、字典等）的方式：

```python
import multiprocessing
import time
import random

def worker(name, shared_dict, shared_list, lock):
    """工作进程"""
    print(f"进程 {name} 开始工作")
    
    # 使用共享字典
    with lock:
        shared_dict[name] = random.randint(1, 100)
        print(f"进程 {name} 更新了共享字典: {shared_dict}")
    
    time.sleep(random.uniform(0.5, 1.5))
    
    # 使用共享列表
    with lock:
        shared_list.append(f"来自 {name} 的数据")
        print(f"进程 {name} 更新了共享列表: {shared_list}")
    
    print(f"进程 {name} 工作完成")

if __name__ == "__main__":
    # 创建manager
    with multiprocessing.Manager() as manager:
        # 创建共享数据结构
        shared_dict = manager.dict()
        shared_list = manager.list()
        
        # 创建锁以确保安全更新
        lock = manager.Lock()
        
        # 创建进程
        processes = []
        for i in range(5):
            p = multiprocessing.Process(
                target=worker, 
                args=(f"Process-{i}", shared_dict, shared_list, lock)
            )
            processes.append(p)
            p.start()
        
        # 等待所有进程完成
        for p in processes:
            p.join()
        
        # 在主进程中查看最终结果
        print("\n最终共享数据:")
        print(f"共享字典: {dict(shared_dict)}")
        print(f"共享列表: {list(shared_list)}")
    
    print("所有进程完成")
```

#### 使用Value和Array共享数据

对于简单的数据类型，可以使用`Value`和`Array`：

```python
import multiprocessing
import time
import random
import os

def worker(name, shared_value, shared_array, lock):
    """工作进程"""
    print(f"进程 {name} (PID: {os.getpid()}) 开始工作")
    
    # 更新共享值
    with lock:
        shared_value.value += 1
        print(f"进程 {name} 更新共享值: {shared_value.value}")
    
    time.sleep(random.uniform(0.5, 1.5))
    
    # 更新共享数组
    with lock:
        index = random.randint(0, len(shared_array) - 1)
        shared_array[index] = shared_value.value
        print(f"进程 {name} 更新共享数组索引 {index}: {list(shared_array)}")
    
    print(f"进程 {name} 工作完成")

if __name__ == "__main__":
    # 创建共享值（整数）
    shared_value = multiprocessing.Value('i', 0)
    
    # 创建共享数组（5个整数）
    shared_array = multiprocessing.Array('i', 5)
    
    # 创建锁以确保安全更新
    lock = multiprocessing.Lock()
    
    # 创建进程
    processes = []
    for i in range(5):
        p = multiprocessing.Process(
            target=worker, 
            args=(f"Process-{i}", shared_value, shared_array, lock)
        )
        processes.append(p)
        p.start()
    
    # 等待所有进程完成
    for p in processes:
        p.join()
    
    # 在主进程中查看最终结果
    print("\n最终共享数据:")
    print(f"共享值: {shared_value.value}")
    print(f"共享数组: {list(shared_array)}")
    
    print("所有进程完成")
```

### 进程池

使用`multiprocessing.Pool`可以轻松管理一组工作进程：

```python
import multiprocessing
import time
import random
import os

def task(name):
    """进程池任务"""
    print(f"任务 {name} 开始执行 (PID: {os.getpid()})")
    time.sleep(random.uniform(0.5, 2.5))  # 随机工作时间
    result = f"任务 {name} 的结果"
    print(f"任务 {name} 执行完成")
    return result

def callback(result):
    """任务完成回调"""
    print(f"回调接收到结果: {result}")

if __name__ == "__main__":
    # 显示CPU核心数
    cpu_count = multiprocessing.cpu_count()
    print(f"本机有 {cpu_count} 个CPU核心")
    
    # 创建进程池，使用所有可用的CPU
    with multiprocessing.Pool(processes=cpu_count) as pool:
        # 方法1: apply_async
        print("\n使用apply_async方法:")
        results = []
        for i in range(10):
            # 提交任务，并注册回调
            result = pool.apply_async(
                task, 
                args=(f"Async-{i}",),
                callback=callback
            )
            results.append(result)
        
        # 获取所有结果
        for result in results:
            value = result.get()  # 等待任务完成并获取结果
            print(f"得到结果: {value}")
        
        # 方法2: map
        print("\n使用map方法:")
        tasks = [f"Map-{i}" for i in range(10)]
        map_results = pool.map(task, tasks)
        
        # 打印结果（按提交顺序）
        for i, result in enumerate(map_results):
            print(f"map结果 {i}: {result}")
        
        # 方法3: imap
        print("\n使用imap方法:")
        tasks = [f"IMap-{i}" for i in range(10)]
        imap_results = pool.imap(task, tasks)
        
        # 迭代获取结果（按提交顺序，但可以在结果可用时立即处理）
        for result in imap_results:
            print(f"imap获取到结果: {result}")
        
        # 方法4: imap_unordered
        print("\n使用imap_unordered方法:")
        tasks = [f"Unordered-{i}" for i in range(10)]
        imap_unordered_results = pool.imap_unordered(task, tasks)
        
        # 迭代获取结果（按完成顺序）
        for result in imap_unordered_results:
            print(f"imap_unordered获取到结果: {result}")
    
    print("\n所有任务完成")
```

### 使用concurrent.futures管理进程

类似于线程池，`concurrent.futures`模块也提供了`ProcessPoolExecutor`：

```python
import concurrent.futures
import time
import random
import os

def task(name):
    """处理任务"""
    print(f"任务 {name} 开始执行 (PID: {os.getpid()})")
    time.sleep(random.uniform(0.5, 2.5))  # 随机工作时间
    result = f"任务 {name} 的结果"
    print(f"任务 {name} 执行完成")
    return result

if __name__ == "__main__":
    # 获取CPU核心数
    cpu_count = os.cpu_count()
    print(f"本机有 {cpu_count} 个CPU核心")
    
    # 创建进程池
    with concurrent.futures.ProcessPoolExecutor(max_workers=cpu_count) as executor:
        # 提交任务
        futures = {executor.submit(task, f"Task-{i}"): i for i in range(10)}
        
        # 获取结果，按完成顺序
        for future in concurrent.futures.as_completed(futures):
            task_id = futures[future]
            try:
                result = future.result()
                print(f"任务 {task_id} 返回结果: {result}")
            except Exception as e:
                print(f"任务 {task_id} 生成异常: {e}")
        
        # 使用map方法
        print("\n使用map方法:")
        tasks = [f"Map-{i}" for i in range(10)]
        results = executor.map(task, tasks)
        
        # 结果按提交顺序返回
        for task_id, result in enumerate(results):
            print(f"任务 {task_id} 的map结果: {result}")
    
    print("\n所有任务完成")
```

## 混合使用线程和进程

在一些复杂的应用中，可能需要混合使用线程和进程：

```python
import multiprocessing
import threading
import time
import random
import os

def thread_worker(name, data):
    """线程工作函数"""
    thread_id = threading.get_ident()
    print(f"线程 {name} (ID: {thread_id}) 在进程 {os.getpid()} 中运行")
    time.sleep(random.uniform(0.5, 1.5))
    print(f"线程 {name} 处理数据: {data}")
    return f"线程 {name} 的结果"

def process_worker(name, data_list):
    """进程工作函数，会创建多个线程"""
    print(f"进程 {name} (PID: {os.getpid()}) 开始创建线程")
    
    # 在进程内创建线程
    threads = []
    for i, data in enumerate(data_list):
        t = threading.Thread(
            target=thread_worker, 
            args=(f"{name}-Thread-{i}", data)
        )
        threads.append(t)
        t.start()
    
    # 等待所有线程完成
    for t in threads:
        t.join()
    
    print(f"进程 {name} 中的所有线程已完成")
    return f"进程 {name} 的结果"

if __name__ == "__main__":
    print(f"主进程 (PID: {os.getpid()}) 开始")
    
    # 准备每个进程的数据
    process_data = [
        [f"Data-1-{i}" for i in range(3)],
        [f"Data-2-{i}" for i in range(4)],
        [f"Data-3-{i}" for i in range(2)]
    ]
    
    # 创建进程
    processes = []
    for i, data_list in enumerate(process_data):
        p = multiprocessing.Process(
            target=process_worker, 
            args=(f"Process-{i}", data_list)
        )
        processes.append(p)
        p.start()
    
    # 等待所有进程完成
    for p in processes:
        p.join()
    
    print("所有进程和线程已完成")
```

## 异步IO与并发编程

Python 3.4引入了`asyncio`模块，提供了使用协程的异步编程方式，这是一种不同于线程和进程的并发模型。

### 基本的异步编程

```python
import asyncio
import time

async def say_after(delay, what):
    """异步协程函数"""
    await asyncio.sleep(delay)  # 非阻塞的睡眠
    print(what)

async def main():
    print(f"开始时间: {time.strftime('%X')}")
    
    # 顺序执行协程
    await say_after(1, '你好')
    await say_after(2, '世界')
    
    print(f"顺序执行完成时间: {time.strftime('%X')}")
    
    # 并发执行协程
    task1 = asyncio.create_task(say_after(1, '你好（并发）'))
    task2 = asyncio.create_task(say_after(2, '世界（并发）'))
    
    # 等待两个任务完成
    await task1
    await task2
    
    print(f"并发执行完成时间: {time.strftime('%X')}")

if __name__ == "__main__":
    asyncio.run(main())
```

### 异步和多线程/多进程的对比

- **线程**：适用于I/O密集型任务，但受GIL限制，不能充分利用多核
- **进程**：适用于CPU密集型任务，可以充分利用多核，但有较大开销
- **异步**：适用于I/O密集型任务，比线程更轻量，但要求代码是异步的

下面是一个对比三种方法的例子：

```python
import time
import threading
import multiprocessing
import asyncio
import concurrent.futures
import requests
import aiohttp
import os

# 共用的URL列表
URLS = [
    'http://httpbin.org/delay/1',
    'http://httpbin.org/delay/1',
    'http://httpbin.org/delay/2',
    'http://httpbin.org/delay/2',
    'http://httpbin.org/delay/3',
]

# 顺序下载
def sequential_download(urls):
    start = time.time()
    results = []
    
    for url in urls:
        print(f"顺序下载: {url}")
        response = requests.get(url)
        results.append(response.status_code)
    
    end = time.time()
    print(f"顺序下载完成，耗时: {end - start:.2f}秒")
    return results

# 线程池下载
def threaded_download(urls):
    start = time.time()
    results = []
    
    def fetch(url):
        print(f"线程下载: {url} (线程ID: {threading.get_ident()})")
        response = requests.get(url)
        return response.status_code
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        future_to_url = {executor.submit(fetch, url): url for url in urls}
        for future in concurrent.futures.as_completed(future_to_url):
            results.append(future.result())
    
    end = time.time()
    print(f"线程池下载完成，耗时: {end - start:.2f}秒")
    return results

# 进程池下载
def process_download(urls):
    start = time.time()
    results = []
    
    def fetch(url):
        print(f"进程下载: {url} (进程ID: {os.getpid()})")
        response = requests.get(url)
        return response.status_code
    
    with concurrent.futures.ProcessPoolExecutor(max_workers=5) as executor:
        future_to_url = {executor.submit(fetch, url): url for url in urls}
        for future in concurrent.futures.as_completed(future_to_url):
            results.append(future.result())
    
    end = time.time()
    print(f"进程池下载完成，耗时: {end - start:.2f}秒")
    return results

# 异步下载
async def async_download(urls):
    start = time.time()
    results = []
    
    async def fetch(url, session):
        print(f"异步下载: {url}")
        async with session.get(url) as response:
            return response.status
    
    async with aiohttp.ClientSession() as session:
        tasks = [fetch(url, session) for url in urls]
        results = await asyncio.gather(*tasks)
    
    end = time.time()
    print(f"异步下载完成，耗时: {end - start:.2f}秒")
    return results

# 主函数
def main():
    print("\n===== 顺序执行 =====")
    sequential_results = sequential_download(URLS)
    
    print("\n===== 线程池执行 =====")
    threaded_results = threaded_download(URLS)
    
    print("\n===== 进程池执行 =====")
    process_results = process_download(URLS)
    
    print("\n===== 异步执行 =====")
    async_results = asyncio.run(async_download(URLS))
    
    print("\n===== 结果比较 =====")
    print(f"顺序结果: {sequential_results}")
    print(f"线程池结果: {threaded_results}")
    print(f"进程池结果: {process_results}")
    print(f"异步结果: {async_results}")

if __name__ == "__main__":
    main()
```

## 多线程和多进程的最佳实践

### 选择线程还是进程

- **使用线程的情况**：
  - I/O密集型任务（如网络请求、文件操作）
  - 需要共享内存数据
  - 对启动速度和资源消耗有要求
  
- **使用进程的情况**：
  - CPU密集型任务（如大量计算）
  - 需要充分利用多核CPU
  - 需要更好的隔离性和稳定性
  
- **使用异步IO的情况**：
  - 高并发I/O操作
  - 需要支持大量并发连接
  - 网络服务器和客户端开发

### 避免常见的并发编程陷阱

1. **死锁**：两个或多个线程/进程互相等待对方释放资源

   预防措施：
   - 对资源获取顺序强制排序
   - 使用超时机制
   - 使用更高级的同步机制（如条件变量）

2. **竞态条件**：结果依赖于线程/进程执行的相对时序

   预防措施：
   - 使用适当的同步机制（锁、信号量等）
   - 使用线程安全的数据结构
   - 避免共享可变状态

3. **资源泄露**：未正确释放资源（如锁、连接、文件等）

   预防措施：
   - 使用上下文管理器（with语句）
   - 使用`try-finally`结构确保资源释放
   - 适当使用超时机制

### 可伸缩性和性能考虑

1. **进程/线程池大小**：
   - 对于CPU密集型任务，进程数通常设置为CPU核心数
   - 对于I/O密集型任务，线程数可以设置得更高（通常是CPU核心数的2-4倍）

2. **负载均衡**：
   - 尽量使任务大小均匀
   - 考虑使用工作队列模式分配任务

3. **避免过度并行**：
   - 考虑同步和通信的开销
   - 监控资源使用情况，避免系统过载

4. **使用性能分析工具**：
   - 使用`cProfile`或其他工具分析瓶颈
   - 测量和比较不同并发策略的性能

### 调试并发代码

1. **使用日志**：
   - 添加详细的日志记录
   - 包含线程/进程ID和时间戳

2. **使用调试工具**：
   - pdb调试器对线程支持有限
   - 考虑使用更专业的并发调试工具

3. **简化复现**：
   - 尝试创建最小可复现的例子
   - 使用确定性的测试案例

## 总结

Python提供了多种并发编程方式，包括多线程、多进程和异步IO，每种方式都有其适用场景：

1. **多线程（threading）**：
   - 共享内存，易于数据共享
   - 受GIL限制，适合I/O密集型任务
   - 提供了丰富的同步原语（Lock, Condition, Event等）

2. **多进程（multiprocessing）**：
   - 独立内存空间，需要特殊机制共享数据
   - 可以绕过GIL，充分利用多核
   - 适合CPU密集型任务
   - 提供了类似threading的API，易于迁移

3. **异步IO（asyncio）**：
   - 单线程协作式多任务
   - 非阻塞I/O操作，高效处理并发
   - 需要使用`async/await`语法
   - 适合I/O密集型任务，尤其是网络应用

选择合适的并发模型对于提高应用性能至关重要。在实际应用中，可能需要混合使用这些技术，以充分发挥各自的优势。

无论选择哪种并发模型，都需要注意正确处理同步、通信和资源管理，避免死锁、竞态条件和资源泄露等常见问题。
